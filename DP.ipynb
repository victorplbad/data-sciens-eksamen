{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Fake News Project\n",
    "# Part 0: Form Study Groups\n",
    "* Alf Kristiansen XGV820\n",
    "* Alexander\n",
    "* Victor\n",
    "* Diego\n",
    "\n",
    "# Part 1: Data Processing\n",
    "### Task 1: Retrieve sample of FakeNewsCorups\n",
    "https://raw.githubusercontent.com/several27/FakeNewsCorpus/master/news_sample.csv\n",
    "\n",
    "Check requirements for version control for our libraries used in this project.\n",
    "- MatPlotLib \n",
    "- scikit-learn\n",
    "- pandas\n",
    "- NLTK\n",
    "- pytorch\n",
    "- numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\alfem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\alfem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "english_stopwords = stopwords.words('english')\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "def full_clean(text: str, stopwords=english_stopwords):\n",
    "    text = text.lower()\n",
    "\n",
    "    text = re.sub(r'\\n', ' ', text) # Remove newlines\n",
    "    text = re.sub(r' +', ' ', text) # Remove multiple spaces\n",
    "\n",
    "    text = re.sub(r'([a-zA-Z]+) (\\d+)[, ]? (\\d{4})', '<DATE>', text) # Date substitution\n",
    "    text = re.sub(r'([.a-zA-Z0-9]+)@([-a-zA-Z0-9]+).([a-zA-Z]+)', '<EMAIL>', text) # E-Mail substitution\n",
    "    text = re.sub(r'(https?:\\/\\/)?(www.)?([-.a-zA-Z0-9]+)[.](co.uk|com|org|net)\\/?([\\%\\-\\.\\?\\_=a-zA-Z0-9\\/]+)?', '<URL>', text) # URL substitution\n",
    "    text = re.sub(r'[0-9]+', '<NUM>', text) # Number substitution\n",
    "\n",
    "    stemmer = PorterStemmer()                                   # Porter Stemmer from nltk\n",
    "    tokens = nltk.word_tokenize(text)                           # Tokenizing the text\n",
    "    tokens = [word for word in tokens if word.isalpha()]        # Removing punctuation\n",
    "    tokens = [word for word in tokens if word not in stopwords] # Removing Stopwords\n",
    "    tokens = [stemmer.stem(word) for word in tokens]            # Stemming all the words\n",
    "    return ' '.join(tokens) # Returning a string consisting of each word in the list\n",
    "\n",
    "def is_credible(article_type):\n",
    "    if article_type in ['fake', 'satire', 'conspiracy', 'bias', 'hate', 'junksci']:\n",
    "        return int(0)\n",
    "    \n",
    "    elif article_type in ['clickbait', 'political', 'reliable']:\n",
    "        return int(1)\n",
    "    \n",
    "    else:\n",
    "        return int(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Apply data pre-processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve sample of FakeNewsCorpus\n",
    "df = pd.read_csv(\"src/995,000_rows.csv\", usecols = [\"type\", \"content\"]) # **WARNING big file, use local directory**\n",
    "\n",
    "# Remove na instances\n",
    "df = df.dropna()\n",
    "\n",
    "# Remove duplicates\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Omit unnessecary types\n",
    "omit_types = ['unreliable', 'unknown', 'rumor', \n",
    "              '2018-02-10 13:43:39.521661']\n",
    "\n",
    "for omit_type in omit_types:\n",
    "    df = df[df.type != omit_type]\n",
    "\n",
    "# Apply full_clean on dataset\n",
    "df[\"content\"] = df[\"content\"].apply(full_clean)\n",
    "\n",
    "# Apply is_credible on dataset\n",
    "df['type'] = df['type'].apply(is_credible)\n",
    "\n",
    "# Save Cleaned Data\n",
    "df.to_csv(\"src/clean_995000_news.csv\", index=False)\n",
    "\n",
    "# Display Sample Results\n",
    "print(df[[\"content\"]].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    type                                            content\n",
      "0      1  plu one articl googl plu thank ali alfoneh ass...\n",
      "1      0  cost best senat bank committe jp morgan buy nu...\n",
      "2      0  man awoken num coma commit suicid learn donald...\n",
      "3      1  julia geist ask draw pictur comput scientist l...\n",
      "4      0  num compil studi vaccin danger activist post n...\n",
      "..   ...                                                ...\n",
      "95     0  jump navig link page namespac main talk user u...\n",
      "96     1  editor miep gie num last hid ann frank famili ...\n",
      "97     0  potenti num gop presidenti candid ben carson s...\n",
      "98     1  back forth fight trump administr court constit...\n",
      "99     1  idea smithsonian latino museum born num task f...\n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Read \"cleaned_995000_news.csv\"\n",
    "new_sample = pd.read_csv(\"src/clean_995000_news.csv\") # **WARNING big file, use local directory**\n",
    "print(new_sample.head(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Exploration of proccesed dataset\n",
    "\n",
    "#### Distribution of types from original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alfem\\AppData\\Local\\Temp\\ipykernel_41952\\1381907806.py:2: DtypeWarning: Columns (0,1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  news = pd.read_csv('src/995,000_rows.csv')\n"
     ]
    }
   ],
   "source": [
    "# Read originial dataset for data exploration\n",
    "news = pd.read_csv('src/995,000_rows.csv') # **WARNING big file, use local directory**\n",
    "omit_types = ['unreliable', 'unknown', 'rumor', \n",
    "              '2018-02-10 13:43:39.521661']\n",
    "\n",
    "for omit_type in omit_types:\n",
    "    news = news[news.type != omit_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'political': 194518,\n",
       " 'fake': 104883,\n",
       " 'satire': 13160,\n",
       " 'reliable': 218564,\n",
       " 'conspiracy': 97314,\n",
       " 'bias': 133232,\n",
       " nan: 47786,\n",
       " 'clickbait': 27412,\n",
       " 'hate': 8779,\n",
       " 'junksci': 14040}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function for counting instances of different types occuring in dataset\n",
    "def count_types(article_types):\n",
    "    found_types = {}\n",
    "\n",
    "    for article_type in article_types:\n",
    "        if article_type in found_types:\n",
    "            found_types[article_type] += 1\n",
    "        else:\n",
    "            found_types[article_type] = 1\n",
    "\n",
    "    return found_types\n",
    "\n",
    "count_types(list(news['type']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the original dataset:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[371408, 440494]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count instances of fake or reliabe news\n",
    "def fake_credible(types: dict):\n",
    "    labels = [0, 0]\n",
    "    \n",
    "    for type in types:\n",
    "        if type in ['fake', 'satire', 'conspiracy', 'bias', 'hate', 'junksci']:\n",
    "            labels[0] += types[type]\n",
    "        elif type in ['clickbait', 'political', 'reliable']:\n",
    "            labels[1] += types[type]\n",
    "\n",
    "    return labels\n",
    "\n",
    "print(\"For the original dataset:\")\n",
    "fake_credible(count_types(list(news['type'])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vocab: frequency of words in cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = []\n",
    "\n",
    "for content in new_sample['content']:\n",
    "    vocab.extend(nltk.word_tokenize(content))\n",
    "\n",
    "vocab = [word for word in vocab if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(vocab):\n",
    "    words = {}\n",
    "\n",
    "    for word in vocab:\n",
    "        if word in words:\n",
    "            words[word] += 1\n",
    "        else:\n",
    "            words[word] = 1\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort vocab from most frequent to least frequent\n",
    "sorted_vocab = {k: v for k, v in sorted(count_words(vocab).items(), key = lambda item: item[1], reverse=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New dataframe for the 10000 most frequent words\n",
    "frequent_words_10k = pd.DataFrame({'word': list(sorted_vocab)[:10000]})\n",
    "# Exporting dataframe to new csv filde\n",
    "frequent_words_10k.to_csv(\"src/frequent_words_10k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num': 7386598,\n",
       " 'said': 1192745,\n",
       " 'one': 869736,\n",
       " 'new': 817397,\n",
       " 'would': 782571,\n",
       " 'time': 781390,\n",
       " 'state': 707026,\n",
       " 'peopl': 704747,\n",
       " 'year': 704408,\n",
       " 'like': 663285,\n",
       " 'also': 534112,\n",
       " 'say': 479711,\n",
       " 'make': 473540,\n",
       " 'use': 470030,\n",
       " 'us': 454982,\n",
       " 'get': 452017,\n",
       " 'go': 438699,\n",
       " 'even': 433049,\n",
       " 'work': 420898,\n",
       " 'govern': 410175,\n",
       " 'american': 403433,\n",
       " 'two': 401558,\n",
       " 'presid': 401314,\n",
       " 'report': 398169,\n",
       " 'could': 396724,\n",
       " 'continu': 392624,\n",
       " 'first': 390600,\n",
       " 'mani': 389326,\n",
       " 'stori': 384640,\n",
       " 'read': 382306,\n",
       " 'nation': 363569,\n",
       " 'right': 359977,\n",
       " 'day': 350528,\n",
       " 'take': 347066,\n",
       " 'world': 344425,\n",
       " 'last': 338565,\n",
       " 'way': 332397,\n",
       " 'think': 330434,\n",
       " 'want': 327039,\n",
       " 'trump': 325470,\n",
       " 'know': 322127,\n",
       " 'call': 321342,\n",
       " 'york': 320009,\n",
       " 'url': 319339,\n",
       " 'may': 318131,\n",
       " 'news': 313463,\n",
       " 'come': 309473,\n",
       " 'countri': 306213,\n",
       " 'need': 305071,\n",
       " 'includ': 299840,\n",
       " 'see': 294350,\n",
       " 'obama': 287914,\n",
       " 'back': 279592,\n",
       " 'much': 277254,\n",
       " 'unit': 274595,\n",
       " 'well': 272231,\n",
       " 'tri': 269833,\n",
       " 'show': 269544,\n",
       " 'polit': 265810,\n",
       " 'support': 265060,\n",
       " 'pleas': 262033,\n",
       " 'thing': 260996,\n",
       " 'date': 259170,\n",
       " 'look': 258806,\n",
       " 'hous': 258145,\n",
       " 'group': 257183,\n",
       " 'compani': 256014,\n",
       " 'main': 254381,\n",
       " 'live': 253663,\n",
       " 'law': 246730,\n",
       " 'made': 245282,\n",
       " 'help': 244207,\n",
       " 'sign': 242338,\n",
       " 'million': 239864,\n",
       " 'public': 238761,\n",
       " 'war': 238701,\n",
       " 'good': 233630,\n",
       " 'percent': 228345,\n",
       " 'still': 227700,\n",
       " 'point': 227459,\n",
       " 'sinc': 226312,\n",
       " 'week': 224707,\n",
       " 'part': 223918,\n",
       " 'power': 222355,\n",
       " 'democrat': 222244,\n",
       " 'chang': 221913,\n",
       " 'servic': 220901,\n",
       " 'citi': 219751,\n",
       " 'republican': 219067,\n",
       " 'offic': 218544,\n",
       " 'gener': 213824,\n",
       " 'plan': 213441,\n",
       " 'newslett': 210303,\n",
       " 'anoth': 208647,\n",
       " 'end': 208356,\n",
       " 'forc': 208349,\n",
       " 'market': 203309,\n",
       " 'must': 201983,\n",
       " 'issu': 200210,\n",
       " 'offici': 198427,\n",
       " 'product': 197652,\n",
       " 'fact': 196946,\n",
       " 'famili': 196659,\n",
       " 'vote': 195379,\n",
       " 'start': 195211,\n",
       " 'follow': 194771,\n",
       " 'case': 194334,\n",
       " 'school': 193906,\n",
       " 'life': 193684,\n",
       " 'accord': 192649,\n",
       " 'place': 192538,\n",
       " 'believ': 192350,\n",
       " 'parti': 191691,\n",
       " 'person': 189506,\n",
       " 'america': 189464,\n",
       " 'elect': 188677,\n",
       " 'offer': 187160,\n",
       " 'ask': 186180,\n",
       " 'recent': 186104,\n",
       " 'everi': 185420,\n",
       " 'home': 184724,\n",
       " 'system': 184676,\n",
       " 'view': 184198,\n",
       " 'secur': 183490,\n",
       " 'white': 182571,\n",
       " 'three': 182262,\n",
       " 'receiv': 181592,\n",
       " 'talk': 181067,\n",
       " 'month': 180231,\n",
       " 'today': 179416,\n",
       " 'play': 178714,\n",
       " 'advertis': 178585,\n",
       " 'around': 177679,\n",
       " 'told': 177471,\n",
       " 'never': 175989,\n",
       " 'long': 175916,\n",
       " 'attack': 174772,\n",
       " 'feder': 174459,\n",
       " 'give': 174009,\n",
       " 'money': 173887,\n",
       " 'в': 173751,\n",
       " 'put': 173292,\n",
       " 'next': 172436,\n",
       " 'member': 171983,\n",
       " 'health': 171925,\n",
       " 'later': 171260,\n",
       " 'found': 170739,\n",
       " 'find': 170470,\n",
       " 'run': 170358,\n",
       " 'number': 170199,\n",
       " 'polic': 169939,\n",
       " 'busi': 169541,\n",
       " 'mean': 168004,\n",
       " 'becom': 167030,\n",
       " 'provid': 166947,\n",
       " 'polici': 165313,\n",
       " 'question': 164899,\n",
       " 'realli': 164264,\n",
       " 'campaign': 164011,\n",
       " 'inform': 163460,\n",
       " 'seem': 162382,\n",
       " 'media': 161580,\n",
       " 'without': 161420,\n",
       " 'senat': 159380,\n",
       " 'major': 159312,\n",
       " 'differ': 158846,\n",
       " 'children': 158388,\n",
       " 'man': 158014,\n",
       " 'human': 157438,\n",
       " 'commun': 156914,\n",
       " 'post': 156832,\n",
       " 'interest': 155146,\n",
       " 'sourc': 154825,\n",
       " 'move': 153815,\n",
       " 'email': 153647,\n",
       " 'organ': 153214,\n",
       " 'bill': 153213,\n",
       " 'subscrib': 153011,\n",
       " 'might': 152705,\n",
       " 'bank': 152526,\n",
       " 'program': 151735,\n",
       " 'court': 151562,\n",
       " 'author': 151504,\n",
       " 'univers': 150670,\n",
       " 'name': 150662,\n",
       " 'happen': 150595,\n",
       " 'turn': 150537,\n",
       " 'keep': 150375,\n",
       " 'open': 149970,\n",
       " 'militari': 149461,\n",
       " 'women': 148893,\n",
       " 'share': 148838,\n",
       " 'ad': 148311,\n",
       " 'someth': 148166,\n",
       " 'job': 147835,\n",
       " 'problem': 147355,\n",
       " 'great': 147034,\n",
       " 'tax': 146953,\n",
       " 'creat': 146848,\n",
       " 'street': 146525,\n",
       " 'order': 146067,\n",
       " 'care': 145743,\n",
       " 'clinton': 144178,\n",
       " 'former': 143106,\n",
       " 'littl': 142604,\n",
       " 'kill': 142536,\n",
       " 'control': 142476,\n",
       " 'game': 141893,\n",
       " 'special': 141825,\n",
       " 'administr': 141565,\n",
       " 'allow': 139976,\n",
       " 'book': 139961,\n",
       " 'claim': 139813,\n",
       " 'set': 139509,\n",
       " 'reason': 139175,\n",
       " 'act': 139093,\n",
       " 'increas': 139049,\n",
       " 'sever': 138207,\n",
       " 'left': 138119,\n",
       " 'agre': 137716,\n",
       " 'articl': 137161,\n",
       " 'result': 136974,\n",
       " 'import': 136745,\n",
       " 'intern': 136259,\n",
       " 'high': 136197,\n",
       " 'washington': 135651,\n",
       " 'let': 135609,\n",
       " 'howev': 135499,\n",
       " 'lead': 134636,\n",
       " 'econom': 134532,\n",
       " 'develop': 134318,\n",
       " 'big': 133825,\n",
       " 'rate': 133758,\n",
       " 'research': 133201,\n",
       " 'appear': 133141,\n",
       " 'free': 132753,\n",
       " 'black': 132104,\n",
       " 'best': 132046,\n",
       " 'far': 130995,\n",
       " 'oper': 130536,\n",
       " 'team': 130304,\n",
       " 'actual': 130087,\n",
       " 'center': 129761,\n",
       " 'thank': 129503,\n",
       " 'base': 129372,\n",
       " 'photo': 129098,\n",
       " 'possibl': 129094,\n",
       " 'leader': 129026,\n",
       " 'video': 128469,\n",
       " 'face': 128341,\n",
       " 'lot': 128229,\n",
       " 'feel': 128110,\n",
       " 'second': 128043,\n",
       " 'address': 127783,\n",
       " 'real': 127486,\n",
       " 'close': 127351,\n",
       " 'better': 126429,\n",
       " 'love': 126301,\n",
       " 'и': 126203,\n",
       " 'respons': 126052,\n",
       " 'less': 126005,\n",
       " 'alreadi': 125789,\n",
       " 'though': 125650,\n",
       " 'rule': 125621,\n",
       " 'studi': 125443,\n",
       " 'effect': 125435,\n",
       " 'current': 125166,\n",
       " 'depart': 125166,\n",
       " 'yet': 125092,\n",
       " 'pay': 124596,\n",
       " 'build': 124486,\n",
       " 'protect': 124333,\n",
       " 'note': 123678,\n",
       " 'th': 123176,\n",
       " 'price': 122725,\n",
       " 'least': 122415,\n",
       " 'john': 122135,\n",
       " 'tell': 121728,\n",
       " 'expect': 121571,\n",
       " 'russia': 121390,\n",
       " 'word': 121197,\n",
       " 'caus': 120777,\n",
       " 'got': 120684,\n",
       " 'deal': 120342,\n",
       " 'among': 119916,\n",
       " 'social': 119593,\n",
       " 'food': 119492,\n",
       " 'fund': 119192,\n",
       " 'meet': 118887,\n",
       " 'hand': 118255,\n",
       " 'hope': 117843,\n",
       " 'de': 117783,\n",
       " 'area': 116949,\n",
       " 'student': 115240,\n",
       " 'investig': 115187,\n",
       " 'posit': 115030,\n",
       " 'stop': 114166,\n",
       " 'record': 113295,\n",
       " 'away': 112799,\n",
       " 'histori': 112713,\n",
       " 'manag': 112642,\n",
       " 'larg': 112177,\n",
       " 'head': 111864,\n",
       " 'enough': 111637,\n",
       " 'global': 111469,\n",
       " 'line': 111385,\n",
       " 'action': 111277,\n",
       " 'billion': 111249,\n",
       " 'remain': 111137,\n",
       " 'past': 111082,\n",
       " 'natur': 110969,\n",
       " 'ago': 110919,\n",
       " 'whether': 110868,\n",
       " 'consid': 110825,\n",
       " 'other': 110458,\n",
       " 'requir': 110181,\n",
       " 'level': 110093,\n",
       " 'occur': 109978,\n",
       " 'updat': 109808,\n",
       " 'cost': 109195,\n",
       " 'alway': 108654,\n",
       " 'pass': 108566,\n",
       " 'water': 108412,\n",
       " 'industri': 108344,\n",
       " 'top': 108277,\n",
       " 'came': 108267,\n",
       " 'death': 108155,\n",
       " 'cours': 108054,\n",
       " 'releas': 107444,\n",
       " 'four': 107076,\n",
       " 'men': 106775,\n",
       " 'bush': 105865,\n",
       " 'took': 105175,\n",
       " 'local': 104902,\n",
       " 'friend': 104388,\n",
       " 'final': 104384,\n",
       " 'financi': 104046,\n",
       " 'event': 103910,\n",
       " 'ever': 103869,\n",
       " 'execut': 103700,\n",
       " 'fight': 103415,\n",
       " 'thought': 103164,\n",
       " 'russian': 102901,\n",
       " 'congress': 102761,\n",
       " 'critic': 102319,\n",
       " 'idea': 102309,\n",
       " 'click': 101881,\n",
       " 'matter': 101485,\n",
       " 'side': 101391,\n",
       " 'win': 101152,\n",
       " 'given': 100986,\n",
       " 'foreign': 100803,\n",
       " 'cut': 100638,\n",
       " 'select': 100302,\n",
       " 'agenc': 100071,\n",
       " 'noth': 99945,\n",
       " 'night': 99763,\n",
       " 'concern': 99357,\n",
       " 'oil': 99352,\n",
       " 'hold': 99326,\n",
       " 'decis': 99248,\n",
       " 'watch': 99232,\n",
       " 'often': 99210,\n",
       " 'comment': 98970,\n",
       " 'effort': 98829,\n",
       " 'director': 98491,\n",
       " 'north': 98340,\n",
       " 'leav': 97890,\n",
       " 'sure': 97747,\n",
       " 'kind': 97711,\n",
       " 'charg': 97684,\n",
       " 'repres': 97650,\n",
       " 'candid': 97173,\n",
       " 'small': 96599,\n",
       " 'hour': 96568,\n",
       " 'list': 96528,\n",
       " 'china': 96501,\n",
       " 'process': 96440,\n",
       " 'energi': 96436,\n",
       " 'statement': 96349,\n",
       " 'conserv': 95434,\n",
       " 'involv': 95351,\n",
       " 'project': 95338,\n",
       " 'press': 95315,\n",
       " 'trade': 95241,\n",
       " 'relat': 95128,\n",
       " 'futur': 95069,\n",
       " 'understand': 94789,\n",
       " 'young': 94633,\n",
       " 'activ': 94421,\n",
       " 'box': 94391,\n",
       " 'known': 94228,\n",
       " 'old': 94117,\n",
       " 'return': 94009,\n",
       " 'serv': 93724,\n",
       " 'begin': 93538,\n",
       " 'economi': 93228,\n",
       " 'associ': 93227,\n",
       " 'educ': 92821,\n",
       " 'rais': 92761,\n",
       " 'account': 91986,\n",
       " 'write': 91967,\n",
       " 'site': 91913,\n",
       " 'went': 91893,\n",
       " 'immigr': 91891,\n",
       " 'form': 91885,\n",
       " 'earli': 91882,\n",
       " 'term': 91742,\n",
       " 'clear': 91597,\n",
       " 'bring': 91525,\n",
       " 'stand': 90574,\n",
       " 'south': 90560,\n",
       " 'within': 90351,\n",
       " 'god': 90323,\n",
       " 'suggest': 90322,\n",
       " 'done': 90309,\n",
       " 'complet': 90224,\n",
       " 'spend': 90178,\n",
       " 'seen': 90147,\n",
       " 'abl': 89314,\n",
       " 'iraq': 89210,\n",
       " 'exampl': 89169,\n",
       " 'almost': 89135,\n",
       " 'defens': 89122,\n",
       " 'fire': 88881,\n",
       " 'learn': 88857,\n",
       " 'die': 88841,\n",
       " 'privat': 88769,\n",
       " 'full': 88324,\n",
       " 'five': 88229,\n",
       " 'announc': 88110,\n",
       " 'legal': 87930,\n",
       " 'union': 87696,\n",
       " 'west': 87648,\n",
       " 'drug': 87128,\n",
       " 'present': 86167,\n",
       " 'discuss': 86036,\n",
       " 'origin': 85295,\n",
       " 'publish': 85252,\n",
       " 'instead': 84860,\n",
       " 'evid': 84775,\n",
       " 'produc': 84677,\n",
       " 'interview': 84637,\n",
       " 'experi': 84479,\n",
       " 'across': 84275,\n",
       " 'error': 84015,\n",
       " 'design': 83775,\n",
       " 'bodi': 83456,\n",
       " 'along': 83423,\n",
       " 'israel': 83396,\n",
       " 'islam': 83144,\n",
       " 'direct': 83018,\n",
       " 'individu': 82584,\n",
       " 'race': 82384,\n",
       " 'perform': 82283,\n",
       " 'friday': 82171,\n",
       " 'rather': 82094,\n",
       " 'voter': 81969,\n",
       " 'muslim': 81817,\n",
       " 'hard': 81802,\n",
       " 'age': 81800,\n",
       " 'season': 81455,\n",
       " 'institut': 81421,\n",
       " 'anyth': 81089,\n",
       " 'corpor': 80692,\n",
       " 'explain': 80401,\n",
       " 'speak': 80247,\n",
       " 'на': 80238,\n",
       " 'protest': 80211,\n",
       " 'worker': 80198,\n",
       " 'daili': 79834,\n",
       " 'buy': 79501,\n",
       " 'christian': 79334,\n",
       " 'fail': 79322,\n",
       " 'test': 79162,\n",
       " 'region': 79109,\n",
       " 'la': 79011,\n",
       " 'dollar': 79001,\n",
       " 'justic': 78785,\n",
       " 'taken': 78780,\n",
       " 'constitut': 78569,\n",
       " 'air': 78339,\n",
       " 'occasion': 78163,\n",
       " 'chief': 78049,\n",
       " 'someon': 77980,\n",
       " 'church': 77901,\n",
       " 'wrote': 77374,\n",
       " 'invest': 77180,\n",
       " 'medic': 77060,\n",
       " 'mind': 76928,\n",
       " 'donald': 76825,\n",
       " 'committe': 76613,\n",
       " 'sunday': 76589,\n",
       " 'reach': 76519,\n",
       " 'east': 76471,\n",
       " 'success': 76429,\n",
       " 'addit': 76414,\n",
       " 'benefit': 76290,\n",
       " 'add': 76252,\n",
       " 'art': 76137,\n",
       " 'gun': 76117,\n",
       " 'total': 75842,\n",
       " 'hear': 75796,\n",
       " 'risk': 75784,\n",
       " 'stock': 75675,\n",
       " 'tuesday': 75584,\n",
       " 'decid': 75583,\n",
       " 'robot': 75458,\n",
       " 'valu': 75451,\n",
       " 'visit': 75335,\n",
       " 'iran': 75180,\n",
       " 'cover': 75129,\n",
       " 'behind': 75072,\n",
       " 'hit': 74877,\n",
       " 'entir': 74854,\n",
       " 'attempt': 74830,\n",
       " 'grow': 74827,\n",
       " 'data': 74636,\n",
       " 'carri': 74538,\n",
       " 'true': 74403,\n",
       " 'late': 74277,\n",
       " 'demand': 74254,\n",
       " 'fall': 74224,\n",
       " 'terrorist': 74168,\n",
       " 'verifi': 74027,\n",
       " 'societi': 73963,\n",
       " 'citizen': 73795,\n",
       " 'liber': 73766,\n",
       " 'toward': 73647,\n",
       " 'bad': 73640,\n",
       " 'imag': 73503,\n",
       " 'page': 73371,\n",
       " 'colleg': 73167,\n",
       " 'began': 73048,\n",
       " 'nuclear': 72841,\n",
       " 'lost': 72756,\n",
       " 'everyon': 72737,\n",
       " 'probabl': 72710,\n",
       " 'anyon': 72646,\n",
       " 'cultur': 72540,\n",
       " 'exist': 72463,\n",
       " 'judg': 72441,\n",
       " 'practic': 72423,\n",
       " 'hillari': 72326,\n",
       " 'civil': 72325,\n",
       " 'land': 72287,\n",
       " 'technolog': 72209,\n",
       " 'car': 72164,\n",
       " 'father': 72110,\n",
       " 'step': 72079,\n",
       " 'arm': 72012,\n",
       " 'limit': 71940,\n",
       " 'monday': 71797,\n",
       " 'crime': 71747,\n",
       " 'syria': 71697,\n",
       " 'togeth': 71660,\n",
       " 'film': 71646,\n",
       " 'whose': 71593,\n",
       " 'light': 71371,\n",
       " 'board': 71282,\n",
       " 'link': 71265,\n",
       " 'train': 71064,\n",
       " 'parent': 70997,\n",
       " 'music': 70948,\n",
       " 'intellig': 70760,\n",
       " 'woman': 70503,\n",
       " 'weapon': 70282,\n",
       " 'simpli': 70039,\n",
       " 'european': 69846,\n",
       " 'propos': 69698,\n",
       " 'minist': 69684,\n",
       " 'save': 69623,\n",
       " 'challeng': 69575,\n",
       " 'invalid': 69242,\n",
       " 'everyth': 69180,\n",
       " 'half': 69128,\n",
       " 'especi': 69085,\n",
       " 'mother': 69077,\n",
       " 'wall': 68978,\n",
       " 'district': 68912,\n",
       " 'held': 68841,\n",
       " 'potenti': 68655,\n",
       " 'threat': 68640,\n",
       " 'thursday': 68556,\n",
       " 'break': 68492,\n",
       " 'truth': 68321,\n",
       " 'per': 68294,\n",
       " 'role': 68078,\n",
       " 'describ': 68034,\n",
       " 'march': 67972,\n",
       " 'popul': 67954,\n",
       " 'decad': 67909,\n",
       " 'outsid': 67773,\n",
       " 'central': 67763,\n",
       " 'answer': 67739,\n",
       " 'freedom': 67659,\n",
       " 'middl': 67581,\n",
       " 'either': 67568,\n",
       " 'detail': 67555,\n",
       " 'nearli': 67468,\n",
       " 'target': 67322,\n",
       " 'counti': 67014,\n",
       " 'wednesday': 66997,\n",
       " 'movement': 66772,\n",
       " 'rise': 66677,\n",
       " 'sale': 66666,\n",
       " 'despit': 66535,\n",
       " 'david': 66353,\n",
       " 'join': 66167,\n",
       " 'fear': 66149,\n",
       " 'capit': 65960,\n",
       " 'room': 65567,\n",
       " 'sens': 65455,\n",
       " 'park': 65124,\n",
       " 'presidenti': 65110,\n",
       " 'lie': 65064,\n",
       " 'player': 65032,\n",
       " 'class': 64989,\n",
       " 'six': 64947,\n",
       " 'europ': 64943,\n",
       " 'accept': 64861,\n",
       " 'whole': 64534,\n",
       " 'figur': 64533,\n",
       " 'although': 64362,\n",
       " 'establish': 64318,\n",
       " 'third': 64293,\n",
       " 'situat': 64268,\n",
       " 'minut': 64239,\n",
       " 'legisl': 64219,\n",
       " 'poll': 64183,\n",
       " 'file': 64176,\n",
       " 'thousand': 64116,\n",
       " 'reader': 64036,\n",
       " 'peac': 64024,\n",
       " 'collect': 63993,\n",
       " 'messag': 63944,\n",
       " 'avail': 63888,\n",
       " 'measur': 63791,\n",
       " 'document': 63739,\n",
       " 'perhap': 63736,\n",
       " 'near': 63505,\n",
       " 'earlier': 63184,\n",
       " 'guy': 63073,\n",
       " 'son': 63032,\n",
       " 'refer': 62995,\n",
       " 'becam': 62916,\n",
       " 'debt': 62874,\n",
       " 'terror': 62794,\n",
       " 'facebook': 62547,\n",
       " 'commit': 62477,\n",
       " 'speech': 62397,\n",
       " 'mark': 62316,\n",
       " 'california': 62180,\n",
       " 'shot': 61905,\n",
       " 'illeg': 61852,\n",
       " 'child': 61850,\n",
       " 'paul': 61700,\n",
       " 'не': 61609,\n",
       " 'stay': 61561,\n",
       " 'seri': 61546,\n",
       " 'debat': 61533,\n",
       " 'led': 61364,\n",
       " 'prevent': 61351,\n",
       " 'push': 61284,\n",
       " 'network': 61190,\n",
       " 'initi': 61188,\n",
       " 'warn': 61154,\n",
       " 'progress': 61140,\n",
       " 'twitter': 61127,\n",
       " 'field': 61036,\n",
       " 'emerg': 61017,\n",
       " 'secretari': 60932,\n",
       " 'higher': 60807,\n",
       " 'space': 60696,\n",
       " 'review': 60600,\n",
       " 'independ': 60597,\n",
       " 'common': 60515,\n",
       " 'insur': 60482,\n",
       " 'seek': 60300,\n",
       " 'quit': 60153,\n",
       " 'due': 60142,\n",
       " 'wrong': 60140,\n",
       " 'strong': 60071,\n",
       " 'ground': 60060,\n",
       " 'armi': 60053,\n",
       " 'amount': 59765,\n",
       " 'saturday': 59626,\n",
       " 'growth': 59521,\n",
       " 'heart': 59482,\n",
       " 'standard': 59391,\n",
       " 'moment': 59390,\n",
       " 'credit': 59265,\n",
       " 'danger': 58991,\n",
       " 'crimin': 58880,\n",
       " 'что': 58796,\n",
       " 'sell': 58592,\n",
       " 'star': 58534,\n",
       " 'lose': 58522,\n",
       " 'reduc': 58462,\n",
       " 'express': 58187,\n",
       " 'short': 58124,\n",
       " 'crisi': 58122,\n",
       " 'access': 58053,\n",
       " 'drive': 57979,\n",
       " 'walk': 57922,\n",
       " 'rememb': 57909,\n",
       " 'singl': 57794,\n",
       " 'insid': 57638,\n",
       " 'travel': 57569,\n",
       " 'gold': 57503,\n",
       " 'similar': 57437,\n",
       " 'georg': 57412,\n",
       " 'els': 57292,\n",
       " 'opportun': 57280,\n",
       " 'front': 57093,\n",
       " 'search': 57033,\n",
       " 'michael': 57012,\n",
       " 'key': 56980,\n",
       " 'defend': 56936,\n",
       " 'scienc': 56901,\n",
       " 'border': 56768,\n",
       " 'kid': 56685,\n",
       " 'condit': 56578,\n",
       " 'consum': 56483,\n",
       " 'goal': 56450,\n",
       " 'check': 56419,\n",
       " 'sound': 56337,\n",
       " 'prepar': 56320,\n",
       " 'miss': 56295,\n",
       " 'reform': 56240,\n",
       " 'connect': 56157,\n",
       " 'longer': 56135,\n",
       " 'arrest': 56087,\n",
       " 'remov': 56058,\n",
       " 'rest': 55955,\n",
       " 'contribut': 55667,\n",
       " 'accus': 55631,\n",
       " 'prison': 55594,\n",
       " 'opposit': 55471,\n",
       " 'averag': 55439,\n",
       " 'period': 55257,\n",
       " 'letter': 55249,\n",
       " 'morn': 55184,\n",
       " 'promis': 55139,\n",
       " 'saw': 55087,\n",
       " 'respect': 55025,\n",
       " 'soon': 55021,\n",
       " 'aid': 54994,\n",
       " 'resid': 54923,\n",
       " 'ye': 54762,\n",
       " 'store': 54740,\n",
       " 'budget': 54709,\n",
       " 'subject': 54688,\n",
       " 'loss': 54505,\n",
       " 'lower': 54303,\n",
       " 'violenc': 54129,\n",
       " 'governor': 54108,\n",
       " 'refus': 53982,\n",
       " 'confer': 53789,\n",
       " 'town': 53773,\n",
       " 'regul': 53734,\n",
       " 'earth': 53630,\n",
       " 'senior': 53412,\n",
       " 'improv': 53362,\n",
       " 'gave': 53325,\n",
       " 'wife': 53312,\n",
       " 'coupl': 53285,\n",
       " 'victim': 53257,\n",
       " 'wait': 53222,\n",
       " 'send': 53148,\n",
       " 'eye': 53017,\n",
       " 'argu': 52964,\n",
       " 'wonder': 52891,\n",
       " 'chanc': 52832,\n",
       " 'featur': 52806,\n",
       " 'religi': 52778,\n",
       " 'choic': 52773,\n",
       " 'employe': 52661,\n",
       " 'reveal': 52641,\n",
       " 'robert': 52613,\n",
       " 'drop': 52605,\n",
       " 'hospit': 52594,\n",
       " 'specif': 52467,\n",
       " 'upon': 52439,\n",
       " 'certain': 52287,\n",
       " 'low': 52237,\n",
       " 'websit': 52228,\n",
       " 'realiti': 52186,\n",
       " 'mass': 52165,\n",
       " 'climat': 52150,\n",
       " 'plant': 52056,\n",
       " 'assist': 51940,\n",
       " 'properti': 51877,\n",
       " 'secret': 51822,\n",
       " 'western': 51806,\n",
       " 'promot': 51795,\n",
       " 'texa': 51718,\n",
       " 'girl': 51670,\n",
       " 'type': 51649,\n",
       " 'respond': 51479,\n",
       " 'gain': 51374,\n",
       " 'mayb': 51237,\n",
       " 'approach': 51172,\n",
       " 'pictur': 51142,\n",
       " 'immedi': 51078,\n",
       " 'piec': 51059,\n",
       " 'red': 50991,\n",
       " 'murder': 50958,\n",
       " 'council': 50945,\n",
       " 'materi': 50934,\n",
       " 'enforc': 50912,\n",
       " 'shoot': 50892,\n",
       " 'agreement': 50842,\n",
       " 'bomb': 50652,\n",
       " 'incom': 50619,\n",
       " 'phone': 50578,\n",
       " 'hundr': 50569,\n",
       " 'attorney': 50550,\n",
       " 'tradit': 50513,\n",
       " 'latest': 50487,\n",
       " 'cent': 50417,\n",
       " 'regard': 50122,\n",
       " 'june': 50086,\n",
       " 'expert': 49986,\n",
       " 'jame': 49978,\n",
       " 'goe': 49971,\n",
       " 'onlin': 49954,\n",
       " 'seriou': 49936,\n",
       " 'popular': 49927,\n",
       " 'sometim': 49923,\n",
       " 'juli': 49887,\n",
       " 'island': 49812,\n",
       " 'ga': 49793,\n",
       " 'firm': 49791,\n",
       " 'paper': 49689,\n",
       " 'british': 49617,\n",
       " 'movi': 49578,\n",
       " 'surpris': 49505,\n",
       " 'approv': 49445,\n",
       " 'compar': 49377,\n",
       " 'sent': 49251,\n",
       " 'road': 49225,\n",
       " 'internet': 49211,\n",
       " 'employ': 49116,\n",
       " 'damag': 48987,\n",
       " 'mention': 48809,\n",
       " 'indic': 48776,\n",
       " 'declin': 48725,\n",
       " 'destroy': 48700,\n",
       " 'al': 48669,\n",
       " 'bit': 48663,\n",
       " 'centuri': 48619,\n",
       " 'reserv': 48584,\n",
       " 'declar': 48468,\n",
       " 'voic': 48465,\n",
       " 'contain': 48378,\n",
       " 'suffer': 48358,\n",
       " 'attent': 48353,\n",
       " 'doctor': 48198,\n",
       " 'cancer': 48178,\n",
       " 'met': 48156,\n",
       " 'conduct': 48123,\n",
       " 'diseas': 48100,\n",
       " 'extrem': 48097,\n",
       " 'advanc': 48013,\n",
       " 'staff': 47984,\n",
       " 'lack': 47857,\n",
       " 'heard': 47835,\n",
       " 'opinion': 47816,\n",
       " 'sexual': 47716,\n",
       " 'particip': 47685,\n",
       " 'determin': 47680,\n",
       " 'primari': 47658,\n",
       " 'foundat': 47560,\n",
       " 'lawyer': 47497,\n",
       " 'pick': 47439,\n",
       " 'ignor': 47413,\n",
       " 'ban': 47366,\n",
       " 'suppli': 47158,\n",
       " 'model': 47106,\n",
       " 'estim': 47066,\n",
       " 'lo': 46964,\n",
       " 'signific': 46837,\n",
       " 'spent': 46699,\n",
       " 'usual': 46692,\n",
       " 'brought': 46661,\n",
       " 'impact': 46653,\n",
       " 'strike': 46628,\n",
       " 'knew': 46610,\n",
       " 'paid': 46487,\n",
       " 'gop': 46411,\n",
       " 'green': 46276,\n",
       " 'confirm': 46219,\n",
       " 'editor': 46184,\n",
       " 'forward': 46160,\n",
       " 'object': 46153,\n",
       " 'enter': 46092,\n",
       " 'earn': 46027,\n",
       " 'appar': 45953,\n",
       " 'syrian': 45903,\n",
       " 'host': 45865,\n",
       " 'victori': 45864,\n",
       " 'patient': 45809,\n",
       " 'professor': 45791,\n",
       " 'pressur': 45788,\n",
       " 'beyond': 45758,\n",
       " 'william': 45690,\n",
       " 'isra': 45671,\n",
       " 'celebr': 45659,\n",
       " 'amend': 45500,\n",
       " 'violat': 45477,\n",
       " 'avoid': 45283,\n",
       " 'oppos': 45188,\n",
       " 'relationship': 45155,\n",
       " 'profit': 45115,\n",
       " 'option': 45091,\n",
       " 'custom': 45062,\n",
       " 'basic': 45038,\n",
       " 'surviv': 45008,\n",
       " 'huge': 44958,\n",
       " 'poor': 44837,\n",
       " 'alleg': 44795,\n",
       " 'с': 44791,\n",
       " 'novemb': 44646,\n",
       " 'equal': 44617,\n",
       " 'contact': 44565,\n",
       " 'stage': 44564,\n",
       " 'king': 44275,\n",
       " 'commiss': 44147,\n",
       " 'dead': 44140,\n",
       " 'agent': 44128,\n",
       " 'resourc': 44086,\n",
       " 'owner': 44033,\n",
       " 'chines': 44003,\n",
       " 'abort': 43944,\n",
       " 'modern': 43904,\n",
       " 'brother': 43795,\n",
       " 'financ': 43715,\n",
       " 'favor': 43685,\n",
       " 'written': 43682,\n",
       " 'televis': 43490,\n",
       " 'encourag': 43438,\n",
       " 'sit': 43429,\n",
       " 'replac': 43418,\n",
       " 'certainli': 43415,\n",
       " 'deni': 43362,\n",
       " 'arriv': 43333,\n",
       " 'marriag': 43297,\n",
       " 'minor': 43226,\n",
       " 'april': 43067,\n",
       " 'purpos': 43040,\n",
       " 'abil': 43016,\n",
       " 'abus': 42987,\n",
       " 'politician': 42965,\n",
       " 'identifi': 42886,\n",
       " 'affect': 42786,\n",
       " 'titl': 42766,\n",
       " 'strategi': 42750,\n",
       " 'conflict': 42741,\n",
       " 'pretti': 42716,\n",
       " 'prove': 42715,\n",
       " 'locat': 42704,\n",
       " 'score': 42684,\n",
       " 'labor': 42667,\n",
       " 'safe': 42638,\n",
       " 'difficult': 42545,\n",
       " 'request': 42472,\n",
       " 'engag': 42435,\n",
       " 'ca': 42422,\n",
       " 'contract': 42344,\n",
       " 'deliv': 42340,\n",
       " 'except': 42191,\n",
       " 'histor': 42162,\n",
       " 'safeti': 42128,\n",
       " 'attend': 42127,\n",
       " 'realiz': 42118,\n",
       " 'block': 41997,\n",
       " 'fox': 41959,\n",
       " 'physic': 41911,\n",
       " 'career': 41847,\n",
       " 'influenc': 41778,\n",
       " 'boy': 41722,\n",
       " 'treat': 41509,\n",
       " 'brown': 41496,\n",
       " 'demonstr': 41433,\n",
       " 'anim': 41385,\n",
       " 'command': 41377,\n",
       " 'particularli': 41357,\n",
       " 'largest': 41349,\n",
       " 'yesterday': 41340,\n",
       " 'previou': 41330,\n",
       " 'suprem': 41274,\n",
       " 'analysi': 41155,\n",
       " 'content': 41126,\n",
       " 'observ': 41124,\n",
       " 'battl': 41061,\n",
       " 'sort': 40997,\n",
       " 'track': 40986,\n",
       " 'struggl': 40925,\n",
       " 'soldier': 40908,\n",
       " ...}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10000 most frequent words\n",
    "frequent_words_10k = dict(list(sorted_vocab.items()))\n",
    "frequent_words_10k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of URLS: 319339\n",
      "Amount of Dates: 259170\n",
      "Amount of Numbers: 7386598\n",
      "Amount of Emails: 153647\n"
     ]
    }
   ],
   "source": [
    "print('Amount of URLS:', sorted_vocab['url'])\n",
    "print('Amount of Dates:', sorted_vocab['date'])\n",
    "print('Amount of Numbers:', sorted_vocab['num'])\n",
    "print('Amount of Emails:', sorted_vocab['email'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(list(range(10000)), list(sorted_vocab.values())[:10000])\n",
    "plt.title('Most frequent words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph of most frequent words follow Zipf's law (which is a powerlaw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: split data\n",
    "We use the sklearn train_test_split function to split the data into the different sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 530841\n",
      "Validation set size: 66356\n",
      "Test set size: 66355\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "new_sample = new_sample[2:]\n",
    "\n",
    "X = new_sample.drop(columns=[\"content\"]).values  # Feature columns\n",
    "y = new_sample[\"type\"].values  # Target column\n",
    "\n",
    "# 80% training and 20% rest for validation and test\n",
    "X_train, X_vt, y_train, y_vt = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)\n",
    "\n",
    "# Split 20% into 10% validation and 10% test\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_vt, y_vt, test_size=0.5, random_state=0, stratify=y_vt)\n",
    "\n",
    "# Print number of samples in each dataset\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Validation set size: {X_val.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Simple Logitstic Regression\n",
    "\n",
    "* Binary classification: `reliabe` or `fake`\n",
    "\n",
    "### Task 0: Label grouping\n",
    "0: `fake`\n",
    "1: `reliable`\n",
    "\n",
    "From part 1 task 1 we've defined a function `is_credible()` and classified the types into reliable or fake by using binary classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Logistic regression classifier\n",
    "* Fixed vocab of 10000 most frequent words.\n",
    "\n",
    "* F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy on validation set:  0.5629869640569664\n",
      "Logistic Regression F1-score on validation set:  0.719918094538992\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import re\n",
    "\n",
    "# 10000 most frequent words\n",
    "freq_words = pd.read_csv('src/frequent_words_10k.csv')\n",
    "\n",
    "def text_to_features(text, vocab):\n",
    "    \"\"\"Convert text into a word frequency vector based on a fixed vocabulary.\"\"\"\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())  # Tokenization using regex\n",
    "    word_counts = Counter(words)  # Count occurrences of each word\n",
    "    return [word_counts.get(word, 0) for word in vocab]  # Keep only vocab words\n",
    "\n",
    "# Apply Function to the Entire Dataset\n",
    "X = np.array([text_to_features(content, freq_words) for content in new_sample[\"content\"]])\n",
    "y = new_sample[\"type\"].values  # Target variable\n",
    "\n",
    "# Train/Test Split (80% Train, 10% Validation, 10% Test)\n",
    "X_train, X_vt, y_train, y_vt = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_vt, y_vt, test_size=0.5, random_state=0, stratify=y_vt)\n",
    "\n",
    "# Train Logistic Regression Model\n",
    "modelLog = LogisticRegression(max_iter=500, solver=\"lbfgs\", C=1.0)\n",
    "modelLog.fit(X_train, y_train)\n",
    "\n",
    "# Make Predictions\n",
    "y_pred = modelLog.predict(X_val)\n",
    "\n",
    "# Evaluate Model Performance\n",
    "accuracy_val = accuracy_score(y_val, y_pred)\n",
    "f1_val = f1_score(y_val, y_pred)\n",
    "\n",
    "# Print Results\n",
    "print(\"Logistic Regression Accuracy on validation set: \", accuracy_val)\n",
    "print(\"Logistic Regression F1-score on validation set: \", f1_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Apply preproccessing pipeline to scraped reliable data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_news = pd.read_csv(\"src/bbc_news.csv\")\n",
    "target_news[\"content\"] = target_news[\"content\"].apply(full_clean)\n",
    "target_news.to_csv(\"src/clean_bbc_news.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             content  prediction\n",
      "0  denmark postal servic postnord end letter deli...           1\n",
      "1  eu leader gather brussel thursday special coun...           1\n",
      "2  summit mark major signific manifest reset rela...           1\n",
      "3  cleaner scrub debat chamber clean mp arriv wed...           1\n",
      "4  british man captur fight ukrain arm forc jail ...           1\n",
      "5  whether ukrain presid volodymyr zelenski decis...           1\n",
      "6  pope franci woke num num local time num num gm...           1\n",
      "7  ten thousand romanian march bucharest victori ...           1\n",
      "8  certainli one way focus mind wallet donald tru...           1\n",
      "9  friedrich merz expect becom germani next chanc...           1\n"
     ]
    }
   ],
   "source": [
    "new_news = pd.read_csv(\"src/clean_bbc_news.csv\")\n",
    "\n",
    "# Transform the \"content\" Column into Word Count Vectors\n",
    "X_new = np.array([text_to_features(content, freq_words) for content in new_news[\"content\"]])\n",
    "\n",
    "# Predict Fake or Reliable\n",
    "new_predictions = modelLog.predict(X_new)\n",
    "\n",
    "# Store Results\n",
    "new_news[\"prediction\"] = new_predictions  # Append predictions to the dataset\n",
    "\n",
    "# Save Predictions to a CSV File\n",
    "new_news.to_csv(\"src/classified_bbc_news.csv\", index=False)\n",
    "\n",
    "# Display Results\n",
    "print(new_news[[\"content\", \"prediction\"]].head(10))  # Show first 10 predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'classified articles fake: 0, and reliable: 672'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classified_news = pd.read_csv(\"src/classified_bbc_news.csv\")\n",
    "classified_news[\"prediction\"]\n",
    "\n",
    "def counting(classified):\n",
    "    reliable = 0\n",
    "    fake = 0\n",
    "\n",
    "    for classification in classified:\n",
    "        if classification == 0 :\n",
    "            fake += 1\n",
    "        elif classification == 1:\n",
    "            reliable += 1\n",
    "\n",
    "    return (f\"classified articles fake: {fake}, and reliable: {reliable}\")\n",
    "\n",
    "counting(list(classified_news['prediction']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Advanced model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Ensuring reproducibility\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Load data frequent words\n",
    "freq_words = pd.read_csv(\"src/frequent_words_10k.csv\")\n",
    "frequent_words = set(freq_words['word'].astype(str).str.strip())\n",
    "\n",
    "# load articles dataset\n",
    "articles = pd.read_csv(r\"C:\\Users\\45422\\Documents\\cleaned_995000_news.csv\") #locally downloaded \"/clean_995000_news.csv\"\n",
    "articles.columns = articles.columns.str.strip()\n",
    "articles['content'] = articles['content'].astype(str)\n",
    "\n",
    "\n",
    "# Filter only top 10000 words\n",
    "def filter_article(text, allowed_words):\n",
    "    tokens = text.split()\n",
    "    return ' '.join([word for word in tokens if word in allowed_words])\n",
    "\n",
    "articles['filtered_content'] = articles['content'].apply(lambda x: filter_article(x, frequent_words))\n",
    "# drop empty \n",
    "articles = articles[articles['filtered_content'].str.strip().astype(bool)].reset_index(drop=True)\n",
    "\n",
    "\n",
    "# TF-IDF vectorization  sparse, every article is a vector of the TF-IDF numerical values only non-zero\n",
    "vectorizer = TfidfVectorizer(max_features=10000)\n",
    "X_sparse = vectorizer.fit_transform(articles['filtered_content'])\n",
    "# label binary values\n",
    "y = articles['type'].astype(int).values\n",
    "\n",
    "\n",
    "# Train/Val/Test split\n",
    "X_train, X_vt, y_train, y_vt = train_test_split(X_sparse, y, test_size=0.2, random_state=0)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_vt, y_vt, test_size=0.5, random_state=0)\n",
    "\n",
    "\n",
    "# Define neural network (inherited form the torch.nn.module class)\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        # Setup in layers\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 512)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(512, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        # Feed forward, how the data flows\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "# Model setup\n",
    "input_size = X_sparse.shape[1]\n",
    "model = NeuralNetwork(input_size)\n",
    "# Error function - binary cross entropy \n",
    "loss_fn = nn.BCELoss()\n",
    "# Adam algorithm for stochastic gradient descent, lr = learning rate, weight_decay = penalty for large weights (prevent overfitting)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005, weight_decay=1e-5)\n",
    "\n",
    "\n",
    "# Model training loop, manual batching, only converting to dense (tensor) when needed\n",
    "# Batch = articles at a time\n",
    "batch_size = 128\n",
    "# Epochs = how many times the model will go though all training data (too many times = over fitting)\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    # Get training data in random order across epochs\n",
    "    permutation = np.random.permutation(X_train.shape[0])\n",
    "\n",
    "    for i in range(0, X_train.shape[0], batch_size):\n",
    "        batch_indices = permutation[i:i + batch_size]\n",
    "        X_batch_sparse = X_train[batch_indices]\n",
    "        y_batch = y_train[batch_indices]\n",
    "\n",
    "        # Sparse to dense (just for batch)\n",
    "        X_batch_dense = torch.tensor(X_batch_sparse.toarray(), dtype=torch.float32)\n",
    "        y_batch_tensor = torch.tensor(y_batch, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(X_batch_dense)\n",
    "        loss = loss_fn(outputs, y_batch_tensor)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()* X_batch_dense.size(0)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_val_tensor = torch.tensor(X_val.toarray(), dtype=torch.float32)\n",
    "        y_val_tensor = torch.tensor(y_val, dtype=torch.float32).view(-1, 1)\n",
    "        val_outputs = model(X_val_tensor)\n",
    "        val_loss = loss_fn(val_outputs, y_val_tensor).item()\n",
    "\n",
    "    avg_train_loss = train_loss / X_train.shape[0]\n",
    "    print(f\"Epoch {epoch + 1}/{epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Evaluation\n",
    "### Task 1: \n",
    "#### Logistic Regression model on test-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy on test set:  0.5630086201699922\n",
      "Logistic Regression F1-score on test set:  0.7198709340855738\n"
     ]
    }
   ],
   "source": [
    "# Make Predictions\n",
    "y_pred = modelLog.predict(X_test)\n",
    "\n",
    "# Evaluate Model Performance\n",
    "accuracy_test = accuracy_score(y_test, y_pred)\n",
    "f1_test = f1_score(y_test, y_pred)\n",
    "\n",
    "# Print Results\n",
    "print(\"Logistic Regression Accuracy on test set: \", accuracy_test)\n",
    "print(\"Logistic Regression F1-score on test set: \", f1_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network model on test-data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate / test \n",
    "with torch.no_grad():\n",
    "    X_test_tensor = torch.tensor(X_test.toarray(), dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "    \n",
    "    y_pred_probs = model(X_test_tensor)\n",
    "    y_pred_labels = (y_pred_probs >= 0.5).float()\n",
    "\n",
    "    accuracy = accuracy_score(y_test_tensor.numpy(), y_pred_labels.numpy())\n",
    "    f1 = f1_score(y_test_tensor.numpy(), y_pred_labels.numpy())\n",
    "\n",
    "    print(f'Test Accuracy: {accuracy:.4f} | f1 score: {f1:.4f}')\n",
    "    print(classification_report(y_test_tensor.numpy(), y_pred_labels.numpy()))\n",
    "    print(confusion_matrix(y_test_tensor.numpy(), y_pred_labels.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2:\n",
    "#### Logistic Regression model on LIAR dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m liar_test \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mliar_dataset/test.tsv\u001b[39m\u001b[38;5;124m'\u001b[39m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \n\u001b[0;32m      2\u001b[0m                         usecols\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m])\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;241m1\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m2\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[0;32m      3\u001b[0m liar_test\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mis_credible_liar\u001b[39m(article_type):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "liar_test = pd.read_csv('liar_dataset/test.tsv', sep='\\t', header=None, \n",
    "                        usecols=[1,2]).rename(columns={1:'type', 2:'content'})\n",
    "liar_test\n",
    "\n",
    "def is_credible_liar(article_type):\n",
    "    if article_type in ['false', 'pants-fire', 'barely-true']:\n",
    "        return 0\n",
    "    elif article_type in ['half-true', 'mostly-true', 'true']:\n",
    "        return 1\n",
    "    \n",
    "liar_test['content'] = liar_test['content'].apply(full_clean)\n",
    "liar_test['type'] = liar_test['type'].apply(is_credible_liar)\n",
    "\n",
    "liar_test.to_csv('src/clean_liar_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liar_test = pd.read_csv('src/clean_liar_test.csv')\n",
    "\n",
    "# Transform the \"content\" Column into Word Count Vectors\n",
    "X_liar = np.array([text_to_features(content, freq_words) for content in liar_test[\"content\"]])\n",
    "y_liar = liar_test[\"type\"].values  # Target variable\n",
    "\n",
    "# Store Results\n",
    "liar_test[\"prediction\"] = new_predictions  # Append predictions to the dataset\n",
    "\n",
    "# Display Results\n",
    "print(liar_test[[\"content\", \"prediction\"]].head(10))  # Show first 10 predictions\n",
    "\n",
    "# Make Predictions\n",
    "y_pred = modelLog.predict(X_liar)\n",
    "\n",
    "# Evaluate Model Performance\n",
    "accuracy_test = accuracy_score(y_liar, y_pred)\n",
    "f1_test = f1_score(y_liar, y_pred)\n",
    "\n",
    "# Print Results\n",
    "print(\"Logistic Regression Accuracy on test set: \", accuracy_test)\n",
    "print(\"Logistic Regression F1-score on test set: \", f1_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network model on LIAR dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Liars dataset test using the model form before\n",
    "liar_articles = pd.read_csv(\"src/clean_liar_test.csv\")\n",
    "liar_articles.columns = liar_articles.columns.str.strip()\n",
    "liar_articles['content'] = liar_articles['content'].astype(str)\n",
    "\n",
    "#filter for 10000 most freq words\n",
    "liar_articles['filtered_liar'] = liar_articles['content'].apply(lambda x: filter_article(x, frequent_words))\n",
    "# drop empty \n",
    "liar_articles = liar_articles[liar_articles['filtered_liar'].str.strip().astype(bool)].reset_index(drop=True)\n",
    "\n",
    "\n",
    "# TF-IDF vectorization  sparse, every article is a vector of the TF-IDF numerical values only non-zero\n",
    "X_sparse_liar = vectorizer.transform(liar_articles['filtered_liar'])\n",
    "# label binary values\n",
    "y_liar = liar_articles['type'].astype(int).values\n",
    "\n",
    "with torch.no_grad():\n",
    "    X_liar_tensor = torch.tensor(X_sparse_liar.toarray(), dtype=torch.float32)\n",
    "    y_liar_tensor = torch.tensor(y_liar, dtype=torch.float32).view(-1, 1)\n",
    "    \n",
    "    y_pred_probs_liar = model(X_liar_tensor)\n",
    "    y_pred_labels_liar = (y_pred_probs_liar >= 0.5).float()\n",
    "\n",
    "    accuracy_liar = accuracy_score(y_liar_tensor.numpy(), y_pred_labels_liar.numpy())\n",
    "    f1_liar = f1_score(y_liar_tensor.numpy(), y_pred_labels_liar.numpy())\n",
    "    \n",
    "    print(f'Test Accuracy: {accuracy_liar:.4f} | f1 score: {f1_liar:.4f}')\n",
    "    print(classification_report(y_liar_tensor.numpy(), y_pred_labels_liar.numpy()))\n",
    "    print(confusion_matrix(y_liar_tensor.numpy(), y_pred_labels_liar.numpy()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GDS_eksamen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
